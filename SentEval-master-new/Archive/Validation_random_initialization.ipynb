{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division\n",
    "\n",
    "import Model\n",
    "import numpy as np \n",
    "#import preTool\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import senteval\n",
    "import pickle\n",
    "import copy\n",
    "import operator\n",
    "import nltk\n",
    "from keras.layers import Dense, Bidirectional,Input, Dropout, Flatten, concatenate, dot, GaussianDropout, Activation, GRU, Embedding\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l1, l2\n",
    "\n",
    "\n",
    "\n",
    "PATH_SENTEVAL = '../'\n",
    "PATH_TO_DATA = 'data'\n",
    "\n",
    "maxLength = 30    \n",
    "wordDim = 300    # 300 \n",
    "num_neurons = [600,600]   # [600,600f]\n",
    "extractLayer = ['X_input_second','X_input_mask_2']\n",
    "kernel_reg = 0.001\n",
    "batch_size = 200\n",
    "merge_mode = 'concat'\n",
    "checkpoint =  \"../randomInitialization.hdf5\"  # Change \n",
    "embeddingPath = \"../Embedding/word2vec.810B.300d.txt\"  # Change \n",
    "processedPath = \"../processed/\"\n",
    "\n",
    "textPath = \"text2/\"\n",
    "wordDicFile = 'wordDic_case.txt'\n",
    "tokenizerFile = 'tokenizer_case.txt'\n",
    "embeddingFile =  'embedding_matrix_case.txt'\n",
    "embeddingFastFile = 'embedding_matrix_fast'\n",
    "valiFile = \"val_set.txt\"\n",
    "\n",
    "model = 0\n",
    "count = 0\n",
    "NonExiste = {}\n",
    "\n",
    "    \n",
    "with open(processedPath + tokenizerFile , \"rb\") as input_file:\n",
    "    t = pickle.load(input_file)\n",
    "    \n",
    "with open(processedPath + embeddingFile , \"rb\") as input_file:\n",
    "    embedding_matrix = pickle.load(input_file)\n",
    "\n",
    "with open(processedPath + embeddingFastFile    , \"rb\") as input_file:\n",
    "    embedding_matrix_fast = pickle.load(input_file)\n",
    "    \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# SentEval prepare and batcher\n",
    "def prepare(params, samples):\n",
    "    return\n",
    "import copy\n",
    "\n",
    "def batcher(params, batch):\n",
    "    global model, count \n",
    "    sentenceList = []\n",
    "    sentences = [' '.join(s) for s in batch]\n",
    "    sentences_2 = copy.deepcopy(sentences)\n",
    "\n",
    "    #embedding_matrix = np.zeros([49985,wordDim])\n",
    "        \n",
    "    sentences = preprocessing(sentences)\n",
    "    sentences = t.texts_to_sequences(sentences)\n",
    "    sentences = pad_sequences(sentences, maxlen=maxLength, padding='post')\n",
    "\n",
    "    \n",
    "    if model == 0 :\n",
    "            print(\"Start loading model\")\n",
    "            model = buildModel(sentences,kernel_reg,num_neurons,merge_mode, len(embedding_matrix),maxLength,wordDim,embedding_matrix,embedding_matrix_fast ,False)\n",
    "            print(\"Loading model\")\n",
    "            model.load_weights(checkpoint)\n",
    "            print(\"Finish build model\")\n",
    "#                     buildModel(input_X_vali,kernel_reg,num_neurons,merge_mode,vocab_size,maxLength,wordDim,embedding_matrix,embedding_matrix_fast,weightTrainable)\n",
    "    #print(\"Extract hidden \")\n",
    "    output_model = extractHiddenState(extractLayer,model,sentences)\n",
    "    output_model = np.mean(np.array(output_model),axis= 0, keepdims = True)\n",
    "    \n",
    "    \n",
    "    for i,sentence in enumerate(sentences_2):\n",
    "        \n",
    "#         sentence = sentence.replace(\"n 't\", \"n't\")\n",
    "        #sentence = sentence.replace(\"-\",\" \")\n",
    "        sentence =  nltk.word_tokenize(sentence)\n",
    "        stopWord = []\n",
    "        nonStopWord = []\n",
    "\n",
    "        for j,vocab in enumerate(sentence):\n",
    "\n",
    "            lower = vocab.lower() \n",
    "            if lower in stop_words:  # has to be lower case \n",
    "                try:\n",
    "                    stopWord.append( list(output_model[0][i][j]))\n",
    "#                     stopWord.append( list(output_model[0][i][j]) + list(output_model[1][i][j]) )\n",
    "                except:\n",
    "                    if vocab not in NonExiste.keys():\n",
    "                        NonExiste[vocab] = 1\n",
    "                    else: \n",
    "                        NonExiste[vocab] += 1 \n",
    "            else:\n",
    "                try:\n",
    "                    nonStopWord.append(list(output_model[0][i][j]))\n",
    "#                     nonStopWord.append(list(output_model[0][i][j]) + list(output_model[1][i][j]))\n",
    "                except:\n",
    "                    if vocab not in NonExiste.keys():\n",
    "                        NonExiste[vocab] = 1\n",
    "                    else: \n",
    "                        NonExiste[vocab] += 1  \n",
    "        #print(\"Stopword:{}, nonStopword:{}, total: {}\".format(len(stopWord),len(nonStopWord),len(sentence)))\n",
    "        if len(stopWord) == 0:\n",
    "            stopWord = np.zeros([1,wordDim*4])\n",
    "        if len(nonStopWord) == 0:\n",
    "            nonStopWord = np.zeros([1,wordDim*4])\n",
    "\n",
    "\n",
    "        average = np.mean(np.array(list(stopWord) + list(nonStopWord)),axis = 0 , keepdims = True)\n",
    "        stopWord = np.mean(np.array(stopWord),axis= 0, keepdims = True)   \n",
    "        nonStopWord = np.mean(np.array(nonStopWord),axis= 0, keepdims = True)\n",
    "        \n",
    "#         min_ = np.amin(np.array(list(stopWord) + list(nonStopWord)), axis=0,keepdims = True)\n",
    "#         max_ = np.amax(np.array(list(stopWord) + list(nonStopWord)), axis=0,keepdims = True)\n",
    "#         output = (list(average[0]) + list(stopWord[0]) + list(nonStopWord[0])+list(min_[0]) +list(max_[0]))\n",
    "        output = list(average[0]) + list(stopWord[0]) + list(nonStopWord[0])\n",
    "        sentenceList.append(output)\n",
    "        #print(len(output))\n",
    "    return sentenceList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRPC\n"
     ]
    }
   ],
   "source": [
    "tasks =  ['MRPC','CR','MR','SUBJ','MR','TREC'] #', 'CR','MPQA',\n",
    "result = []\n",
    "for task in tasks:\n",
    "    print(task)\n",
    "    \n",
    "    params_senteval = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 10}\n",
    "    params_senteval['classifier'] = {'nhid': 0, 'optim': 'adam', 'batch_size':256,\n",
    "                                     'tenacity': 5, 'epoch_size': 4}\n",
    "    # Set up logger\n",
    "    se = senteval.engine.SE(params_senteval, batcher, prepare)\n",
    "    transfer_tasks =    [task]  # ['CR','MPQA','TREC','MRPC']\n",
    "    # 'MR', 'CR' ,'MPQA', 'SUBJ','TREC', 'MRPC','SICKEntailment', 'SICKRelatedness'  ,'TREC','MRPC'\n",
    "    results = se.eval(transfer_tasks)\n",
    "    print(results)\n",
    "    result.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model = extractHiddenState(extractLayer,model,sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(output_model).shape\n",
    "x = np.mean(np.array(output_model),axis= 0, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(input_X,kernel_reg,num_neurons,merge_mode, vocab_size,max_length, wordDim,embedding_matrix,embedding_matrix_2,trainable):\n",
    "    # Create shared layer for masked sentence \n",
    "    embedding_1 = Embedding(vocab_size, wordDim, weights=[embedding_matrix], input_length=max_length, trainable=trainable ,name = \"embedding\")\n",
    "    embedding_2 = Embedding(vocab_size, wordDim, weights=[embedding_matrix_2], input_length=max_length, trainable=trainable ,name = \"embedding_fast\")\n",
    "    shared_second = Bidirectional(GRU(num_neurons[1],kernel_initializer = 'RandomUniform',return_sequences = True,kernel_regularizer=l2(kernel_reg)), merge_mode= merge_mode,name='X_input_mask_2')\n",
    "\n",
    "    org_input = Input(shape=(input_X.shape[1],), name = 'org_input')\n",
    "    emb_org = embedding_1(org_input)\n",
    "    org_input_2 = Bidirectional(GRU(num_neurons[1],kernel_initializer = 'Orthogonal',return_sequences = True, kernel_regularizer=l2(kernel_reg)), merge_mode= merge_mode,name='X_input_second')(emb_org)\n",
    "\n",
    "    mask_input_first = Input(shape=(input_X.shape[1],), name = 'mask_input_first')\n",
    "    emb_first = embedding_2(mask_input_first)\n",
    "    mask_input_first_2 = shared_second(emb_first)\n",
    "\n",
    "    mask_input_second = Input(shape=(input_X.shape[1],), name = 'mask_input_second')\n",
    "    emb_second = embedding_2(mask_input_second)\n",
    "    mask_input_second_2 = shared_second(emb_second)\n",
    "\n",
    "    mask_input_third = Input(shape=(input_X.shape[1],), name = 'mask_input_third')\n",
    "    emd_third = embedding_2(mask_input_third)\n",
    "    mask_input_third_2 = shared_second(emd_third)\n",
    "\n",
    "    out1 = dot([org_input_2, mask_input_first_2], axes=1, normalize = True,name='output_1')\n",
    "    out2 = dot([org_input_2, mask_input_second_2], axes=1,normalize = True,name='output_2')\n",
    "    out3 = dot([org_input_2, mask_input_third_2], axes=1, normalize = True,name='output_3')\n",
    "\n",
    "    concat = concatenate([out1, out2, out3], name='concat')\n",
    "\n",
    "    output = Activation('softmax')(concat)\n",
    "\n",
    "    model = Model(inputs=[org_input,mask_input_first,mask_input_second,mask_input_third], outputs=output)\n",
    "\n",
    "    return model\n",
    "\n",
    "def extractHiddenState(layerName,model,predict_input):\n",
    "    \n",
    "    outputs = []\n",
    "\n",
    "    for i in range(len(layerName)):\n",
    "        outputs.append(model.get_layer(layerName[i]).get_output_at(0))\n",
    "\n",
    "    input_layer = Model(inputs=model.input,\n",
    "                        outputs=outputs)\n",
    "\n",
    "    input_layer_embed = input_layer.predict([predict_input,predict_input,predict_input,predict_input])\n",
    "    \n",
    "    return input_layer_embed\n",
    "\n",
    "def preprocessing(lines): \n",
    "    #print(\"Pre processing Data\")\n",
    "  \n",
    "    for line in lines:\n",
    "        # Decapitalize: Conver to lower case first. \n",
    "        line = line.lower()\n",
    "        # Delete url \n",
    "#         line = re.sub(r\"http\\S+\", \"link\", line)\n",
    "#         line = re.sub(r\"\\S+html\", \"link\", line)\n",
    "#         line = re.sub(r\"\\S+.com$\", \"link\", line)\n",
    "#         line = re.sub(r\"\\S+.jpg$\", \"photo\", line)\n",
    "#         #line = decontracted(line)\n",
    "        '''\n",
    "        ignore:  * { } \\  < > \n",
    "        Splite based on : ! . ,  &  # ' $ \n",
    "        line = re.findall(r\"[\\w']+|[().,:!?;'$&]\", line)\n",
    "        '''\n",
    "        #line = re.findall(r\"[\\w']+|[().,:!?;$&]\", line)\n",
    "        line = line.split()\n",
    " \n",
    "    return lines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
